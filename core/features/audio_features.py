from __future__ import annotations
from typing import Dict, Any, Union
import numpy as np
import librosa
import tempfile
import os

# ============================================================
# ğŸ“Œ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ø§Ù„ØµÙˆØªÙŠ
# ============================================================
def load_audio(file_path: str, sr: int = 16000):
    """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ø§Ù„ØµÙˆØªÙŠ ÙˆØªØ­ÙˆÙŠÙ„Ù‡ Ø¥Ù„Ù‰ Mono Ù…Ø¹ Ù…Ø¹Ø¯Ù„ Ø¹ÙŠÙ†Ø§Øª Ø«Ø§Ø¨Øª."""
    signal, sr = librosa.load(file_path, sr=sr, mono=True)
    return signal, sr

# ============================================================
# ğŸ”¹ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙŠØ²Ø§Øª Ø§Ù„ØµÙˆØª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
# ============================================================
def extract_energy(signal: np.ndarray) -> float:
    """Ø­Ø³Ø§Ø¨ Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„ÙƒÙ„ÙŠØ© Ù„Ù„ØµÙˆØª (RMS Energy)."""
    return float(np.sqrt(np.mean(signal ** 2)))

def extract_pitch(signal: np.ndarray, sr: int) -> float:
    """ØªÙ‚Ø¯ÙŠØ± Ø·Ø¨Ù‚Ø© Ø§Ù„ØµÙˆØª (Pitch) Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Autocorrelation."""
    pitches, magnitudes = librosa.piptrack(y=signal, sr=sr)
    pitch_values = pitches[magnitudes > np.median(magnitudes)]
    return float(np.median(pitch_values)) if len(pitch_values) > 0 else 0.0

def extract_speech_rate(signal: np.ndarray, sr: int) -> float:
    """ØªÙ‚Ø¯ÙŠØ± Ù…Ø¹Ø¯Ù„ Ø§Ù„ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Zero Crossing Rate."""
    zcr = librosa.feature.zero_crossing_rate(signal)
    avg_zcr = float(np.mean(zcr))
    return avg_zcr * sr / 1000.0  # Ù…Ù‚Ø§Ø·Ø¹/Ø«Ø§Ù†ÙŠØ©

# ============================================================
# ğŸ”¹ Ø­Ø³Ø§Ø¨ Ù…Ø¤Ø´Ø± Ø§Ù„ØªÙˆØªØ± Ø§Ù„Ø¹Ø§Ù…
# ============================================================
def compute_stress_index(energy: float, pitch: float, speech_rate: float) -> float:
    """
    Ù…Ø¤Ø´Ø± Ø§Ù„ØªÙˆØªØ±: Ù‚ÙŠØ§Ø³ Ù…Ø¨Ø¯Ø¦ÙŠ ÙŠØ¬Ù…Ø¹ Ø¨ÙŠÙ† Ø§Ù„Ø·Ø§Ù‚Ø© + Ø·Ø¨Ù‚Ø© Ø§Ù„ØµÙˆØª + Ù…Ø¹Ø¯Ù„ Ø§Ù„ÙƒÙ„Ø§Ù….
    Ù†Ø·Ø§Ù‚ Ø§Ù„Ù‚ÙŠÙ… ~ [0, 100].
    """
    stress = (energy * 50) + (pitch / 300.0 * 30) + (speech_rate * 20)
    return round(min(100, stress), 2)

# ============================================================
# ğŸ”¹ Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØª (Ù…Ø³ØªØ®Ø¯Ù…Ø© ÙÙŠ Insight Engineering)
# ============================================================
def analyze_audio(file_obj: Union[str, bytes]) -> Dict[str, Any]:
    """
    ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ø§Ù„ØµÙˆØªÙŠ ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©:
    - Ø§Ù„Ù…Ø¯Ø© Ø§Ù„Ø²Ù…Ù†ÙŠØ©
    - Ù…ØªÙˆØ³Ø· Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø·Ø§Ù‚Ø©
    - Ø§Ù„Ù†ØºÙ…Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© (Pitch)
    - Ø·Ø§Ù‚Ø© Ø§Ù„Ø·ÙŠÙ Spectral Energy
    """
    try:
        # Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ BytesIO Ù…Ù† Streamlit
        if not isinstance(file_obj, str):
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
                tmp.write(file_obj.read())
                tmp_path = tmp.name
            file_path = tmp_path
        else:
            file_path = file_obj

        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ø§Ù„ØµÙˆØªÙŠ
        y, sr = librosa.load(file_path, sr=None)

        # Ø·ÙˆÙ„ Ø§Ù„Ù…Ù„Ù Ø¨Ø§Ù„Ø«ÙˆØ§Ù†ÙŠ
        duration = librosa.get_duration(y=y, sr=sr)

        # Ù…ØªÙˆØ³Ø· Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø·Ø§Ù‚Ø© (Root Mean Square Energy)
        rms = np.mean(librosa.feature.rms(y=y))

        # Ø§Ù„Ø·ÙŠÙ Ø§Ù„Ø·Ø§Ù‚ÙŠ
        spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))

        # ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ù†ØºÙ…Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© (Pitch)
        pitches, magnitudes = librosa.piptrack(y=y, sr=sr)
        pitch = np.mean(pitches[magnitudes > np.median(magnitudes)]) if np.any(magnitudes) else 0

        # ØªÙ‚Ø¯ÙŠØ± Ù†Ø³Ø¨Ø© Ø§Ù„ØµÙ…Øª
        silence_ratio = np.mean(y == 0) if len(y) > 0 else 0

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
        emotion_analysis = analyze_audio_emotions(file_path)

        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø¤Ù‚ØªØ©
        if not isinstance(file_obj, str):
            os.remove(file_path)

        # Ù…Ø¤Ø´Ø±Ø§Øª Ø¬Ø§Ù‡Ø²Ø© Ù„Ù„Ø¥Ø±Ø¬Ø§Ø¹
        return {
            "duration_sec": round(duration, 2),
            "rms_energy": round(float(rms), 4),
            "spectral_centroid": round(float(spectral_centroid), 2),
            "pitch_hz": round(float(pitch), 2),
            "silence_ratio": round(float(silence_ratio), 3),
            "emotion_analysis": emotion_analysis
        }

    except Exception as e:
        return {
            "error": str(e),
            "duration_sec": None,
            "rms_energy": None,
            "spectral_centroid": None,
            "pitch_hz": None,
            "silence_ratio": None,
            "emotion_analysis": None
        }

# ============================================================
# ğŸ”¹ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ø§Ø·ÙÙŠ Ù„Ù„ØµÙˆØª (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
# ============================================================
def analyze_audio_emotions(file_path: str) -> Dict[str, Any]:
    """
    ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…ÙŠØ²Ø§Øª Ø§Ù„ØµÙˆØª:
    - Ø§Ù„Ø·Ø§Ù‚Ø©
    - Ø·Ø¨Ù‚Ø© Ø§Ù„ØµÙˆØª
    - Ù…Ø¹Ø¯Ù„ Ø§Ù„ÙƒÙ„Ø§Ù…
    - Ù…Ø¤Ø´Ø± Ø§Ù„ØªÙˆØªØ±
    - Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: Ø¥ÙŠØ¬Ø§Ø¨ÙŠ / Ù…Ø­Ø§ÙŠØ¯ / Ø³Ù„Ø¨ÙŠ
    """
    try:
        signal, sr = load_audio(file_path)
        energy = extract_energy(signal)
        pitch = extract_pitch(signal, sr)
        speech_rate = extract_speech_rate(signal, sr)
        stress_idx = compute_stress_index(energy, pitch, speech_rate)

        if stress_idx < 35:
            sentiment = "positive"
        elif stress_idx < 65:
            sentiment = "neutral"
        else:
            sentiment = "negative"

        return {
            "energy": round(energy, 3),
            "pitch": round(pitch, 2),
            "speech_rate": round(speech_rate, 2),
            "stress_index": stress_idx,
            "sentiment": sentiment
        }

    except Exception as e:
        return {"error": str(e)}
